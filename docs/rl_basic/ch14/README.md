## 思考解答

### 第 8 章 练习题

1. $\text{DQN}$ 算法为什么会产生 $Q$ 值的过估计问题？

$\qquad$ 答：原因主要有：**数据相关性**：每次更新神经网络时，使用的都是之前采集到的数据，这些数据之间存在相关性。这导致神经网络的训练过程不稳定，可能会导致 $Q$ 值的过估计问题。**最大化操作**：$\text{DQN}$ 算法在更新目标 $Q$ 值时，使用的是当前神经网络在下一个状态下具有最大 $Q$ 值的动作。这种最大化操作可能会导致某些状态和动作的 $Q$ 值被过估计。为了解决这个问题，可以采用一些技术，如 $\text{Double DQN}$ 和 $\text{Dueling DQN}$ 。$\text{Double DQN}$ 通过使用一个神经网络来估计当前状态下各个动作的 $Q$ 值，使用另一个神经网络来计算目标 $Q$ 值，从而减少Q值的过估计问题。$\text{Dueling DQN}$ 则通过将 $Q$ 值分解为状态值和优势值两部分，从而更准确地估计 $Q$ 值，减少 $Q$ 值的过估计问题。这些技术可以有效地减少 $Q$ 值的过估计问题，提高 $\text{DQN}$ 算法的性能。

2. 同样是提高探索，$\text{Noisy DQN}$ 和  $\varepsilon-\text{greedy}$ 策略 有什么区别？

$\qquad$ 答： $\varepsilon-\text{greedy}$ 策略是一种基于概率的探索策略，其思想是在每个时间步中，以概率 $\varepsilon$ 选择一个随机动作，以概率 $1-\varepsilon$ 选择当前状态下具有最大Q值的动作。当随机动作被选择时，智能体有一定的概率探索新的状态和动作，从而提高探索能力。$\varepsilon-\text{greedy}$ 策略的优点是简单易用，但可能存在随机性过高或过低的问题，影响探索效果。$\text{Noisy DQN}$ 是一种基于网络权重噪声的探索策略，其思想是在神经网络中添加一定的权重噪声，以增加探索的随机性。在每个时间步中，神经网络中的权重噪声会随机地改变神经元的输出，从而改变智能体选择动作的概率分布。$\text{Noisy DQN}$ 的优点是能够自适应地控制探索随机性的大小，从而更加有效地提高探索能力。

### 第 9 章 练习题

1. 基于价值和基于策略的算法各有什么优缺点？
   
$\qquad$ 答：前者的优点有：**简单易用**：通常只需要学习一个值函数，往往收敛性也会更好。保守更新：更新策略通常是隐式的，通过更新价值函数来间接地改变策略，这使得学习可能更加稳定。缺点有：**受限于离散动作**；**可能存在多个等价最优策略**：当存在多个等效的最优策略时，基于价值的方法可能会在它们之间不停地切换。后者的优点有：直接优化策略：由于这些算法直接操作在策略上，所以它们可能更容易找到更好的策略；适用于连续动作空间；**更高效的探索**：通过调整策略的随机性，基于策略的方法可能会有更高效的探索策略。缺点有：**高方差**：策略更新可能会带来高方差，这可能导致需要更多的样本来学习。**可能会收敛到局部最优**：基于策略的方法可能会收敛到策略的局部最优，而不是全局最优，且收敛较缓慢。在实践中，还存在结合了基于价值和基于策略方法的算法，即 $\text{Actor-Critic}$ 算法，试图结合两者的优点来克服各自的缺点。选择哪种方法通常取决于具体的应用和其特点。

2. 马尔可夫平稳分布需要满足什么条件？

$\qquad$ 答：**状态连通性**：从任何一个状态可以在有限的步数内到达另一个状态；**非周期性**：由于马尔可夫链需要收敛，那么就一定不能是周期性的。

3. $\text{REINFORCE}$ 算法会比 $\text{Q-learning}$ 算法训练速度更快吗？为什么？

$\qquad$ 答：两者的速度不能一概而论，尽管前者往往会比后者慢。主要考虑几个因素：**样本效率**：因为 $\text{REINFORCE}$ 算法是一个无偏的估计，但其方差可能很高，这意味着为了得到一个稳定和准确的策略更新，它可能需要与环境交互更多的样本，如果与环境交互的成本很高，$\text{REINFORCE}$ 算法将会显得更加劣势。**稳定性与收敛**：$\text{Q-learning}$ 和其他基于值的方法，特别是当与深度神经网络结合时，可能会遇到训练不稳定的问题。这可能会影响其训练速度。

4. 确定性策略与随机性策略的区别？

$\qquad$ 答：对于同一个状态，确定性策略会给出一个明确的、固定的动作，随机性策略则会为每一个可能的动作（legal action）提供一个概率分布。前者在训练中往往需要额外的探索策略，后者则只需要调整动作概率。但前者相对更容易优化，因为不需要考虑所有可能的动作，但也容易受到噪声的影响。后者则相对更加鲁棒，适用面更广，因为很多的实际问题中，我们往往无法得到一个确定的最优策略，而只能得到一个概率分布，尤其是在博弈场景中。

### 第 10 章 练习题

1. 相比于 $\text{REINFORCE}$ 算法， $\text{A2C}$ 主要的改进点在哪里，为什么能提高速度？

$\qquad$ 答：改进点主要有：**优势估计**：可以更好地区分好的动作和坏的动作，同时减小优化中的方差，从而提高了梯度的精确性，使得策略更新更有效率；**使用 $\text{Critic}$** ： $\text{REINFORCE}$ 通常只使用 $\text{Actor}$ 网络，没有 $\text{Critic}$ 来辅助估计动作的价值，效率更低；**并行化**：即 $\text{A3C}$ ，允许在不同的环境中并行运行多个 $\text{Agent}$，每个 $\text{Agent}$ 收集数据并进行策略更新，这样训练速度也会更快。

2. $\text{A2C}$ 算法是 $\text{on-policy}$ 的吗？为什么？

$\qquad$ 答： $\text{A2C}$ 在原理上是一个 $\text{on-policy}$ 算法，首先它使用当前策略的样本数据来更新策略，然后它的优势估计也依赖于当前策略的动作价值估计，并且使用的也是策略梯度方法进行更新，因此是 $\text{on-policy}$ 的。但它可以被扩展为支持 $\text{off-policy}$ 学习，比如引入经验回放，但注意这可能需要更多的调整，以确保算法的稳定性和性能。

### 第 11 章 练习题

1. $\text{DDPG}$ 算法是 $\text{off-policy}$ 算法吗？为什么？

$\qquad$ 答：跟 $\text{DQN}$ 一样，$\text{DDPG}$ 算法，主要结合了经验回放、目标网络和确定性策略，是典型的 $\text{off-policy}$ 算法。

2. 软更新相比于硬更新的好处是什么？为什么不是所有的算法都用软更新？

$\qquad$ 答：好处：**平滑目标更新**：软更新通过逐渐调整目标网络的参数，使其向主网络的参数靠近，而不是直接复制主网络的参数。这样做可以降低目标的变化幅度，减少了训练中的不稳定性；**降低方差**；**避免振荡**：软更新可以减少目标网络和主网络之间的振荡，这有助于更稳定地收敛到良好的策略。缺点：**速度**：软更新会使目标网络变得更加缓慢；**探索和稳定性权衡**：一些算法可能更倾向于使用硬更新，因为它们可能需要更频繁地探索新的策略，而不依赖于过去的经验。硬更新允许在每次更新时完全用新策略替代旧策略；**算法需求**：某些算法可能对硬更新更敏感，而且硬更新可能是这些算法的关键组成部分。综上，软更新和硬更新都有其用途，选择哪种方式取决于具体的问题和算法要求。

3. 相比于 $\text{DDPG}$ 算法，$\text{TD3}$ 算法做了哪些改进？请简要归纳。

$\qquad$ 答：**双Q网络**： $\text{TD3}$ 使用了两个独立的 $\text{Q}$ 网络，分别用于估计动作的价值。这两个Q网络有不同的参数，这有助于减少估计误差，并提高了训练的稳定性；**目标策略噪声**：与 $\text{DDPG}$ 不同，$\text{TD3}$ 将噪声添加到目标策略，而不是主策略。这有助于减小动作值的过估计误差；**目标策略平滑化**：$\text{TD3}$ 使用目标策略平滑化技术，通过对目标策略的参数进行软更新来减小目标策略的变化幅度。这有助于提高稳定性和训练的收敛性。**延迟策略更新**：$\text{TD3}$ 引入了延迟策略更新，意味着每隔一定数量的时间步才更新主策略网络。这可以减小策略更新的频率，有助于减少过度优化的风险，提高稳定性。

4. $\text{TD3}$ 算法中 $\text{Critic}$ 的更新频率一般要比 $\text{Actor}$ 是更快还是更慢？为什么？

$\qquad$ 答：$\text{Critic}$ 网络的更新频率要比 $\text{Actor}$ 网络更快，即延迟策略更新。延迟策略更新的目的是减小策略更新的频率，以避免过度优化和提高训练的稳定性。因为 $\text{Critic}$ 网络的更新频率更高，它可以更快地适应环境的变化，提供更准确的动作价值估计，从而帮助 $\text{Actor}$ 网络生成更好的策略。


### 第 12 章 练习题

1. 为什么 $\text{DQN}$ 和 $\text{DDPG}$ 算法不使用重要性采样技巧呢？

$\qquad$ 答：$\text{DQN}$ 和 $\text{DDPG}$ 是 $\text{off-policy}$ 算法，它们通常不需要重要性采样来处理不同策略下的采样数据。相反，它们使用目标网络和优势估计等技巧来提高训练的稳定性和性能。

2. $\text{PPO}$ 算法原理上是 $\text{on-policy}$ 的，但它可以是 $\text{off-policy}$ 的吗，或者说可以用经验回放来提高训练速度吗?为什么？（提示：是可以的，但条件比较严格）

$\qquad$ 答：跟 $\text{A2C}$ 一样，可以将经验回放与 $\text{PPO}$ 结合，创建一个 $\text{PPO with Experience Replay (PPO-ER)}$ 算法。在 $\text{PPO-ER}$ 中，智能体使用经验回放缓冲区中的数据来训练策略网络，这样可以提高训练效率和稳定性。这种方法通常需要调整PPO的损失函数和采样策略，以适应 $\text{off-policy}$ 训练的要求，需要谨慎调整。

3. $\text{PPO}$ 算法更新过程中在将轨迹样本切分个多个小批量的时候，可以将这些样本顺序打乱吗？为什么？

$\qquad$ 答：将轨迹样本切分成多个小批量时，通常是可以将这些样本顺序打乱的，这个过程通常称为样本随机化（ $\text{sample shuffling}$ ），这样做的好处有降低样本相关性、减小过拟合风险以及增加训练多样性（更全面地提高探索空间）。

4. 为什么说重要性采样是一种特殊的蒙特卡洛采样？

$\qquad$ 答：原因有：**估计期望值**：蒙特卡洛方法的核心目标之一是估计一个随机变量的期望值。蒙特卡洛采样通过从分布中生成大量的样本，并求取这些样本的平均值来估计期望值。重要性采样也是通过从一个分布中生成样本，但不是均匀地生成样本，而是按照另一个分布的权重生成样本，然后使用这些带权重的样本来估计期望值。**改进采样效率**：重要性采样的主要目的是改进采样效率。当我们有一个难以从中采样的分布时，可以使用重要性采样来重新调整样本的权重，以使估计更准确。这类似于在蒙特卡洛采样中调整样本大小以提高估计的精确性。**权重分布**：在重要性采样中，我们引入了一个额外的权重分布，用于指导采样过程。这个权重分布决定了每个样本的相对贡献，以确保估计是无偏的。在蒙特卡洛采样中，权重通常是均匀分布，而在重要性采样中，权重由分布的比率（要估计的分布和采样分布之间的比例）决定。