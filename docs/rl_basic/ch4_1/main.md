## 时序差分估计

时序差分估计方法是一种基于经验的动态规划方法，它结合了蒙特卡洛和动态规划的思想。最简单的时序差分可以表示为式 $\text{(4.5)}$ 。

$$
\tag{4.5}
V(s_t) \leftarrow V(s_t) + \alpha[r_{t+1}+\gamma V(s_{t+1})- V(s_{t})]
$$

这种算法一般称为**单步时序差分**（ $\text{one-step TD}$ ），即 $TD(0)$。可以看到，在这个更新过程中使用了当前奖励和后继状态的估计，这是类似于蒙特卡罗方法的；但同时也利用了贝尔曼方程的思想，将下一状态的值函数作为现有状态值函数的一部分估计来更新现有状态的值函数。此外，时序差分还结合了自举（ $\text{bootstrap}$ ）的思想，即未来状态的价值是通过现有的估计 $r_{t+1}+\gamma V(s_{t+1})$ （也叫做**时序差分目标**）进行计算的，即使用一个状态的估计值来更新该状态的估计值，没有再利用后续状态信息的计算方法。这种方法的好处在于可以将问题分解成只涉及一步的预测，从而简化计算。此外，$\delta=r_{t+1}+\gamma V(s_{t+1})- V(s_{t})$被定义为 **时序差分误差**（ $\text{TD error}$ ）。

但有一点需要注意的是，由于基于时步的学习方式，并且终止状态没有下一步，比如当 $V(s_{t})$ 是终止状态时，$\gamma V(s_{t+1})$ 是没有意义的。因此时序差分方法在实践过程中会把终止状态单独做一个判断，即将对应未来状态的估计值设置为 $0$，然后更新当前状态的估计值，这个过程也被称作**回溯**，如式 $\text{(4.6)}$ 所示，后面所有基于时序差分的方法都会有这样的一个判断。

$$
\tag{4.6}
\begin{split}
    \begin{cases} V(s_t) \leftarrow V(s_t) + \alpha[r_{t+1}- V(s_{t})]
 & \text {对于终止状态} V(s_{t}) \\ V(s_t) \leftarrow V(s_t) + \alpha[r_{t+1}+\gamma V(s_{t+1})- V(s_{t})] & \text {对于非终止状态} V(s_{t})\end{cases}
\end{split}
$$

## 时序差分和蒙特卡洛的比较

结合图 $4.4$ 总结一下时序差分方法和蒙特卡洛方法之间的差异。

* 时序差分方法可以在线学习（ $\text{online learning}$ ），每走一步就可以更新，效率高。蒙特卡洛方法必须等游戏结束时才可以学习。
* 时序差分方法可以从不完整序列上进行学习。蒙特卡洛方法只能从完整的序列上进行学习。
* 时序差分方法可以在连续的环境下（没有终止）进行学习。蒙特卡洛方法只能在有终止的情况下学习。
* 时序差分方法利用了马尔可夫性质，在马尔可夫环境下有更高的学习效率。蒙特卡洛方法没有假设环境具有马尔可夫性质，利用采样的价值来估计某个状态的价值，在不是马尔可夫的环境下更加有效。


<div align=center>
<img width="600" src="figs/TD_3.png"/>
</div>
<div align=center>图 $4.4$ 时序差分方法和蒙特卡洛方法的差异</div>

##  n 步时序差分

把时序差分方法进一步拓展，之前只是向前自举了一步，即 $\text{TD(0)}$ ，我们可以调整为两步，利用两步得到的回报来更新状态的价值，调整 $n$ 步就是 $n$ 步时序差分（$\text{n-step TD}$），如式 $\text{4.7}$ 所示。

$$
\tag{4.7}
\begin{aligned}
& n=1(\mathrm{TD}) \quad G_t^{(1)}=r_{t+1}+\gamma V\left(s_{t+1}\right) \\
& n=2 \quad G_t^{(2)}=r_{t+1}+\gamma r_{t+2}+\gamma^2 V\left(s_{t+2}\right) \\
& n=\infty(\mathrm{MC}) \quad G_t^{\infty}=r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{T-t-1} r_T \\
&
\end{aligned}
$$

我们会发现当 $n$ 趋近于无穷大时，就变成了蒙特卡洛方法，因此可以通过调整自举的步数，来实现蒙特卡洛方法和时序差分方法之间的权衡。这个 $n$ 我们通常会用 
$\lambda$ 来表示，即 $\text{TD}(\lambda)$ 方法。

以下是一些常见的方法来选择合适的 $\lambda$ 。

* 网格搜索（ $\text{Grid Search}$ ）：对于给定的一组 $\lambda$ 值，可以通过网格搜索方法在这些值中进行遍历，并评估每个值对应的算法性能。选择在验证集上表现最好的 $\lambda$ 值作为最终的选择。

* 随机搜索（ $\text{Random Search}$ ）：随机选择一组 $\lambda$ 值，在验证集上评估每个值对应的算法性能。通过多次随机搜索，可以得到更好的 $\lambda$ 值。

* 自适应选择：在训练过程中逐渐适应地调整 $\lambda$ 的取值。例如，可以在训练的早期使用较小的 $\lambda$ 值，以更多地依赖单步TD误差来减小偏差；在训练的后期逐渐增大 $\lambda$ 值，以更多地依赖多步回报来减小方差。

* 交叉验证（ $\text{Cross-validation}$ ）：将数据集划分为多个子集，交叉验证不同的 $\lambda$ 值，并平均它们的性能评估结果。这样可以更好地估计不同 $\lambda$ 值的泛化性能。

* 经验取值：在某些情况下，根据先前的经验或已知的任务特性，可以选择一些常用的 $\lambda$ 取值作为初始值，并进一步微调。

需要注意的是，无论使用哪种方法，$\lambda$ 的最佳取值可能因任务、环境和算法的不同而异。因此，选择合适的 $\lambda$ 值是一个实验过程，需要根据具体问题进行调整。在实际应用中，可以结合多种方法来找到最佳的 $\lambda$ 值，以获得更好的性能。

## 思考

**蒙特卡洛和时序差分都是无模型方法吗？**

是的，蒙特卡洛方法和时序差分方法都是无模型（$\text{model-free}$）方法。这意味着它们不需要对环境的动态模型进行显式建模，而是通过与环境的交互来学习价值函数或策略。

**蒙特卡洛方法和时序差分方法的优劣势分别是什么？**

**蒙特卡洛方法优势**：可以直接从经验中学习，不需要环境的转移概率；收敛性良好，可以保证在有限步内收敛到最优策略；可以处理长期回报，对于无折扣情况也可以使用。**蒙特卡洛方法劣势**：需要等到一条完整的轨迹结束才能更新价值函数，因此效率较低；对于连续状态空间和动作空间的问题，蒙特卡洛方法难以处理。**时序差分优势**：可以在交互的过程中逐步更新价值函数，效率较高；可以处理连续状态空间和动作空间的问题；可以结合函数逼近方法使用，对于高维状态空间的问题有很好的应用。**时序差分劣势**：更新过程中存在一定的方差，可能会影响收敛速度和稳定性；对于无折扣情况，需要采取一些特殊的方法来保证收敛。总的来说，蒙特卡洛方法对于离散状态空间的问题，特别是存在长期回报的问题有很好的适用性，但是其效率较低。时序差分方法则可以高效地处理连续状态空间和动作空间的问题，但是其更新过程中存在方差问题。在实际应用中需要根据问题的特点和实际情况选择合适的方法。